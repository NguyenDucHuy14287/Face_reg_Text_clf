{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "82af821f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.preprocessing.image import img_to_array\n",
    "from keras.preprocessing.image import load_img\n",
    "import cv2\n",
    "\n",
    "from keras.applications import inception_v3\n",
    "from keras.applications.inception_v3 import InceptionV3\n",
    "from keras.applications.inception_v3 import preprocess_input as inception_v3_preprocessor\n",
    "\n",
    "from keras.layers import Dense, GlobalAveragePooling2D\n",
    "from keras.models import Model\n",
    "\n",
    "from keras.optimizers import Adam\n",
    "from keras.metrics import categorical_crossentropy\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from tqdm import tqdm\n",
    "import keras\n",
    "from os import makedirs\n",
    "from os.path import expanduser, exists, join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "440a6e2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Version 1.2\n",
      "Device mapping:\n",
      "/job:localhost/replica:0/task:0/device:XLA_CPU:0 -> device: XLA_CPU device\n",
      "/job:localhost/replica:0/task:0/device:XLA_GPU:0 -> device: XLA_GPU device\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\LEGION\\Desktop\\train_final\\class_image_classify_model.py:31: The name tf.keras.backend.set_session is deprecated. Please use tf.compat.v1.keras.backend.set_session instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from class_image_classify_model import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4f717a4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f38f9f0",
   "metadata": {},
   "source": [
    "# Load meta data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "eebf4f38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_path</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>images/7081912075039165723_7100.jpeg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>images/6998898528495979803_8750.jpeg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>images/7081261598936960282_3750.jpeg</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>images/7033757107044306177_750.jpeg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>images/7079000650503163162_3250.jpeg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39908</th>\n",
       "      <td>images/7092751126881258778_4071.jpeg</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39909</th>\n",
       "      <td>images/7031012310688877850_315.jpeg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39910</th>\n",
       "      <td>images/7020713309389294849_00.jpeg</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39911</th>\n",
       "      <td>images/7009563299805138203_250.jpeg</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39912</th>\n",
       "      <td>images/7106287816166001946_1985.jpeg</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>39913 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 image_path  label\n",
       "0      images/7081912075039165723_7100.jpeg      0\n",
       "1      images/6998898528495979803_8750.jpeg      0\n",
       "2      images/7081261598936960282_3750.jpeg      1\n",
       "3       images/7033757107044306177_750.jpeg      0\n",
       "4      images/7079000650503163162_3250.jpeg      0\n",
       "...                                     ...    ...\n",
       "39908  images/7092751126881258778_4071.jpeg      1\n",
       "39909   images/7031012310688877850_315.jpeg      0\n",
       "39910    images/7020713309389294849_00.jpeg      1\n",
       "39911   images/7009563299805138203_250.jpeg      1\n",
       "39912  images/7106287816166001946_1985.jpeg      1\n",
       "\n",
       "[39913 rows x 2 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_label_raw = pd.read_csv('train_set_1_image_shuffle.csv', usecols=['label', 'image_path'])\n",
    "train_label_raw"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5265c908",
   "metadata": {},
   "source": [
    "# Load data and train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "68a47435",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total row: 39913\n"
     ]
    }
   ],
   "source": [
    "total_row = train_label_raw.shape[0]\n",
    "print('Total row:', total_row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f208d15e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== Load model successful from model_checkpoint/checkpoint_30000_35000.h5\n",
      "===== Load images...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44b8c124b5b5481da269f04937f77d07",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4913 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "256/256 [==============================] - 112s 436ms/step - loss: 0.2718 - accuracy: 0.8879 - val_loss: 0.2823 - val_accuracy: 0.8875\n",
      "Epoch 2/10\n",
      "256/256 [==============================] - 110s 429ms/step - loss: 0.2569 - accuracy: 0.8957 - val_loss: 0.3043 - val_accuracy: 0.8562\n",
      "Epoch 3/10\n",
      "256/256 [==============================] - 117s 455ms/step - loss: 0.2359 - accuracy: 0.9078 - val_loss: 0.2770 - val_accuracy: 0.8906\n",
      "Epoch 4/10\n",
      "256/256 [==============================] - 111s 433ms/step - loss: 0.2477 - accuracy: 0.8981 - val_loss: 0.3163 - val_accuracy: 0.8625\n",
      "Epoch 5/10\n",
      "256/256 [==============================] - 113s 440ms/step - loss: 0.2252 - accuracy: 0.9105 - val_loss: 0.3142 - val_accuracy: 0.8844\n",
      "Epoch 6/10\n",
      "256/256 [==============================] - 113s 442ms/step - loss: 0.2438 - accuracy: 0.8973 - val_loss: 0.2515 - val_accuracy: 0.9062\n",
      "Epoch 7/10\n",
      "256/256 [==============================] - 110s 431ms/step - loss: 0.2187 - accuracy: 0.9066 - val_loss: 0.2932 - val_accuracy: 0.8594\n",
      "Epoch 8/10\n",
      "256/256 [==============================] - 112s 437ms/step - loss: 0.2059 - accuracy: 0.9117 - val_loss: 0.3117 - val_accuracy: 0.8687\n",
      "Epoch 9/10\n",
      "256/256 [==============================] - 115s 450ms/step - loss: 0.2007 - accuracy: 0.9173 - val_loss: 0.2926 - val_accuracy: 0.8813\n",
      "Epoch 10/10\n",
      "256/256 [==============================] - 116s 451ms/step - loss: 0.2026 - accuracy: 0.9219 - val_loss: 0.2852 - val_accuracy: 0.8750\n",
      "===== Saved model.\n"
     ]
    }
   ],
   "source": [
    "start_idx = 35000\n",
    "end_idx = 39913\n",
    "num_ep = 10\n",
    "steps_per_epoch = 256\n",
    "validation_steps = 32\n",
    "\n",
    "trainer = image_binary_classify_keras()\n",
    "checkpoint_path = 'model_checkpoint/checkpoint_30000_35000.h5'\n",
    "trainer.init_model(checkpoint_path=checkpoint_path)\n",
    "# trainer.init_model()\n",
    "\n",
    "train_label_df = train_label_raw.iloc[start_idx:end_idx]\n",
    "trainer.generate_train_valid(img_path_arr=train_label_df['image_path'], label_arr=train_label_df['label'])\n",
    "trainer.train_model(num_epochs=num_ep, steps_per_epoch=steps_per_epoch, validation_steps=validation_steps)\n",
    "\n",
    "# save model\n",
    "os.makedirs('model_checkpoint', exist_ok=True)\n",
    "trainer.save_model(f'model_checkpoint/checkpoint_{start_idx}_{end_idx}.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f046ca97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "518"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "62169621",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# index_lst = [(0, 5000), (5000, 10000), (10000, 15000), (15000, 20000), (20000, 25000), (25000, 30000), (30000, 35000), (35000, 39913)]\n",
    "# for i, tup_idx in enumerate(index_lst):\n",
    "#     if i != 0:\n",
    "#         start_idx = tup_idx[0]\n",
    "#         end_idx = tup_idx[1]\n",
    "#         num_ep = 10\n",
    "#         steps_per_epoch = 256\n",
    "#         validation_steps = 32\n",
    "\n",
    "#         trainer = image_binary_classify_keras()\n",
    "#         checkpoint_path = f'model_checkpoint/checkpoint_{index_lst[i-1][0]}_{index_lst[i-1][1]}.h5'\n",
    "#         trainer.init_model(checkpoint_path=checkpoint_path)\n",
    "# #         trainer.init_model()\n",
    "\n",
    "#         train_label_df = train_label_raw.iloc[start_idx:end_idx]\n",
    "#         trainer.generate_train_valid(img_path_arr=train_label_df['image_path'], label_arr=train_label_df['label'])\n",
    "#         trainer.train_model(num_epochs=num_ep, steps_per_epoch=steps_per_epoch, validation_steps=validation_steps)\n",
    "\n",
    "#         # save model\n",
    "#         os.makedirs('model_checkpoint', exist_ok=True)\n",
    "#         trainer.save_model(f'model_checkpoint/checkpoint_{start_idx}_{end_idx}.h5')\n",
    "#         gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6dcdd4e",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f4f4da8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.load_model(\"model_checkpoint/checkpoint_35000_39913.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "58cade59",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set_2_image = pd.read_csv('train_set_2_image.csv')\n",
    "test_set_image = pd.read_csv('test_set_image.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "30fcbc66",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(27115, 3)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set_2_image.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "f94d68be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12643, 3)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_set_image.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "07f679f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2643, 3)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test = test_set_image.iloc[10000:12643]\n",
    "df_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "958ac2a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "710"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "bdc6f26b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2643/2643 [00:22<00:00, 115.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "83/83 - 99s\n"
     ]
    }
   ],
   "source": [
    "# Creae the x_test\n",
    "x_test = []\n",
    "for image_path in tqdm(df_test['image_path'].values):\n",
    "     img = cv2.imread(image_path)\n",
    "     x_test.append(cv2.resize(img, (299, 299)))\n",
    "\n",
    "# Make it an array\n",
    "x_test = np.array(x_test, np.float32) / 255.\n",
    "\n",
    "# Predict x_test\n",
    "predictions = model.predict(x_test, verbose=2)\n",
    "\n",
    "# Set column names to those generated by the one-hot encoding earlier\n",
    "# col_names = one_hot.columns.values\n",
    "col_names = np.array([0, 1])\n",
    "\n",
    "# Create the submission data.\n",
    "submission_results = pd.DataFrame(predictions, columns = col_names)\n",
    "\n",
    "# Add the id as the first column\n",
    "submission_results.insert(0, 'label', df_test.reset_index()['label'])\n",
    "submission_results.insert(0, 'id', df_test.reset_index()['id'])\n",
    "submission_results.insert(0, 'image_path', df_test.reset_index()['image_path'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "c4fd9820",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_path</th>\n",
       "      <th>id</th>\n",
       "      <th>label</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>images/6988467376165506305_500.jpeg</td>\n",
       "      <td>6988467376165506305</td>\n",
       "      <td>0</td>\n",
       "      <td>0.994877</td>\n",
       "      <td>0.005779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>images/6988467376165506305_5000.jpeg</td>\n",
       "      <td>6988467376165506305</td>\n",
       "      <td>0</td>\n",
       "      <td>0.967974</td>\n",
       "      <td>0.032380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>images/6988467376165506305_4250.jpeg</td>\n",
       "      <td>6988467376165506305</td>\n",
       "      <td>0</td>\n",
       "      <td>0.992729</td>\n",
       "      <td>0.008728</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>images/6988467376165506305_00.jpeg</td>\n",
       "      <td>6988467376165506305</td>\n",
       "      <td>0</td>\n",
       "      <td>0.998430</td>\n",
       "      <td>0.001538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>images/6988467376165506305_3000.jpeg</td>\n",
       "      <td>6988467376165506305</td>\n",
       "      <td>0</td>\n",
       "      <td>0.986167</td>\n",
       "      <td>0.015408</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2638</th>\n",
       "      <td>images/6847699753216576769_00.jpeg</td>\n",
       "      <td>6847699753216576769</td>\n",
       "      <td>0</td>\n",
       "      <td>0.997876</td>\n",
       "      <td>0.001886</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2639</th>\n",
       "      <td>images/6847699753216576769_535.jpeg</td>\n",
       "      <td>6847699753216576769</td>\n",
       "      <td>0</td>\n",
       "      <td>0.709060</td>\n",
       "      <td>0.191781</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2640</th>\n",
       "      <td>images/6847699753216576769_250.jpeg</td>\n",
       "      <td>6847699753216576769</td>\n",
       "      <td>0</td>\n",
       "      <td>0.971020</td>\n",
       "      <td>0.018625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2641</th>\n",
       "      <td>images/7031018951089933594_90.jpeg</td>\n",
       "      <td>7031018951089933594</td>\n",
       "      <td>0</td>\n",
       "      <td>0.233384</td>\n",
       "      <td>0.741816</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2642</th>\n",
       "      <td>images/7031018951089933594_00.jpeg</td>\n",
       "      <td>7031018951089933594</td>\n",
       "      <td>0</td>\n",
       "      <td>0.773377</td>\n",
       "      <td>0.162531</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2643 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                image_path                   id  label  \\\n",
       "0      images/6988467376165506305_500.jpeg  6988467376165506305      0   \n",
       "1     images/6988467376165506305_5000.jpeg  6988467376165506305      0   \n",
       "2     images/6988467376165506305_4250.jpeg  6988467376165506305      0   \n",
       "3       images/6988467376165506305_00.jpeg  6988467376165506305      0   \n",
       "4     images/6988467376165506305_3000.jpeg  6988467376165506305      0   \n",
       "...                                    ...                  ...    ...   \n",
       "2638    images/6847699753216576769_00.jpeg  6847699753216576769      0   \n",
       "2639   images/6847699753216576769_535.jpeg  6847699753216576769      0   \n",
       "2640   images/6847699753216576769_250.jpeg  6847699753216576769      0   \n",
       "2641    images/7031018951089933594_90.jpeg  7031018951089933594      0   \n",
       "2642    images/7031018951089933594_00.jpeg  7031018951089933594      0   \n",
       "\n",
       "             0         1  \n",
       "0     0.994877  0.005779  \n",
       "1     0.967974  0.032380  \n",
       "2     0.992729  0.008728  \n",
       "3     0.998430  0.001538  \n",
       "4     0.986167  0.015408  \n",
       "...        ...       ...  \n",
       "2638  0.997876  0.001886  \n",
       "2639  0.709060  0.191781  \n",
       "2640  0.971020  0.018625  \n",
       "2641  0.233384  0.741816  \n",
       "2642  0.773377  0.162531  \n",
       "\n",
       "[2643 rows x 5 columns]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "d677b675",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_results.to_csv('test_set_image_result_2.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85600bc4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "53d2a56c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set_2_image_result_df = pd.concat([pd.read_csv(f'train_set_2_image_result_{i}.csv') for i in range(6)])\n",
    "test_set_image_result_df = pd.concat([pd.read_csv(f'test_set_image_result_{i}.csv') for i in range(3)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "57e9a0be",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set_2_image_result_df.to_csv('train_set_2_image_result.csv', index=False)\n",
    "test_set_image_result_df.to_csv('test_set_image_result.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "cc3767e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12643, 5)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_set_image_result_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "4e58aef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# new_df = train_label_raw.sample(frac=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "81a68971",
   "metadata": {},
   "outputs": [],
   "source": [
    "# new_df.to_csv('train_set_1_image_shuffle.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "966844e8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
